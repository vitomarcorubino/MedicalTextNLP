{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Medical Text\n",
    "\n",
    "Medical Text Dataset [https://www.kaggle.com/datasets/chaitanyakck/medical-text/data]"
   ],
   "id": "d0ec552398d1b4fc"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-13T10:38:02.358574Z",
     "start_time": "2025-02-13T10:37:48.701959Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "import nltk\n",
    "from spacy import displacy\n",
    "import spacy\n",
    "from transformers import pipeline\n",
    "import pickle\n",
    "import json\n",
    "import os\n",
    "import stanza\n",
    "from itertools import product"
   ],
   "id": "d99e0d16279e752b",
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-13T10:38:02.558582Z",
     "start_time": "2025-02-13T10:38:02.374762Z"
    }
   },
   "cell_type": "code",
   "source": "df = pd.read_csv('data/train.dat', sep=\"\\t\", header=None)",
   "id": "eb4726daef7573a7",
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-13T10:38:04.629018Z",
     "start_time": "2025-02-13T10:38:04.611780Z"
    }
   },
   "cell_type": "code",
   "source": [
    "df.rename(columns={0:'condition', 1:'abstract'}, inplace=True)\n",
    "df.head()"
   ],
   "id": "c7fd949f8b9fe3d2",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "   condition                                           abstract\n",
       "0          4  Catheterization laboratory events and hospital...\n",
       "1          5  Renal abscess in children. Three cases of rena...\n",
       "2          2  Hyperplastic polyps seen at sigmoidoscopy are ...\n",
       "3          5  Subclavian artery to innominate vein fistula a...\n",
       "4          4  Effect of local inhibition of gamma-aminobutyr..."
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>condition</th>\n",
       "      <th>abstract</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>4</td>\n",
       "      <td>Catheterization laboratory events and hospital...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>5</td>\n",
       "      <td>Renal abscess in children. Three cases of rena...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>Hyperplastic polyps seen at sigmoidoscopy are ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>5</td>\n",
       "      <td>Subclavian artery to innominate vein fistula a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>Effect of local inhibition of gamma-aminobutyr...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 3
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "In order to perform NLP we decided to not include records labelled as \"general pathological conditions\" due to the amount of data we have. The remaining dataset will provide a sufficient amount of information.",
   "id": "ce31e0ec3fbae121"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-13T10:38:07.734765Z",
     "start_time": "2025-02-13T10:38:07.719790Z"
    }
   },
   "cell_type": "code",
   "source": "df.drop(index=df[df[\"condition\"]==5].index,inplace=True)",
   "id": "69181626982ff0a5",
   "outputs": [],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-13T10:38:11.266119Z",
     "start_time": "2025-02-13T10:38:11.249904Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# replace each number with the corresponding condition\n",
    "mapping = {\n",
    "    'condition': {1: \"neoplasm\",\n",
    "                  2: \"digestive system disease\",\n",
    "                  3: \"nervous system disease\",\n",
    "                  4: \"cardiovascular disease\"}\n",
    "}\n",
    "\n",
    "df.replace(mapping, inplace = True)"
   ],
   "id": "a76e8d46abdfae38",
   "outputs": [],
   "execution_count": 5
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# display changes\n",
    "df.head()"
   ],
   "id": "ac11f12593454778"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Lexical Analysis\n",
    "Lexical analysis consists of the operations of:\n",
    "- **Sentence Splitting**: The technique aims to identify the beginning and end of a textual fragment (sentence or clause) with informative content, even if simple. To achieve this, it uses orthographic features of words (e.g., uppercase initial letters) and delimiters (e.g., punctuation).\n",
    "- **Tokenization**: The goal of tokenization is to pinpoint the starting and ending positions of each token, whether itâ€™s a word, a number, or a combination of symbols. As with sentence splitting, the process relies on orthographic features (e.g., initial capital letters) and delimiters (e.g., punctuation).\n",
    "- **Lemmatization**: Post-tokenization techniques address the morphological analysis of word-tokens. Lemmatization identifies the base form (lemma) of inflected words, preserving their meaning and grammatical category. For example, the token _liked_ maps to the lemma *like*. This process minimizes lexical variation by consolidating different forms of the same word into a unified representation.\n",
    "- **Stemming**: Like lemmatization, processes inflected forms but reduces them to their root, which may not correspond to a dictionary word. Unlike lemmatization, it focuses on inflections that create new words and may change the grammatical class, such as *probable* (adjective) stemming to *probably* (adverb).\n",
    "- **POS Tagging**: Part-of-speech (POS) tagging assigns a grammatical category to each token, such as noun, verb, or adjective\n",
    "\n",
    "It focuses on the main components of a text (words), and aims to recognize them in relation to the context in which they are used, such as sentences or clauses."
   ],
   "id": "64fe66e2cae9b9a4"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "#### Funzioni utili\n",
    "From now on there will be repetitive steps in order to check files existence prior to create each one: the following functions aims to simplify the process."
   ],
   "id": "23278753ffcab3f5"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-13T10:38:17.585380Z",
     "start_time": "2025-02-13T10:38:17.579937Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def check_existence(obj):\n",
    "    \"\"\"\n",
    "    checks if we already stored the file\n",
    "    Arg:\n",
    "    obj = str name of the object\n",
    "    \"\"\"\n",
    "    path = 'objects/'+obj+'.pkl'\n",
    "    if os.path.exists(path):\n",
    "        with open(path, 'rb') as file:\n",
    "            file = pickle.load(file)\n",
    "            return file\n",
    "    else:\n",
    "        return False"
   ],
   "id": "600f15822e686bce",
   "outputs": [],
   "execution_count": 6
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-13T10:38:19.550649Z",
     "start_time": "2025-02-13T10:38:19.544462Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def save_step(name, obj):\n",
    "    \"\"\"\n",
    "    Saves object to pickle and json files\n",
    "    Args:\n",
    "    name = 'obj_name'\n",
    "    obj = object to save\n",
    "    \"\"\"\n",
    "\n",
    "    with open('objects/'+name+'.pkl', 'wb') as file:\n",
    "        pickle.dump(obj, file)\n",
    "\n",
    "    with open('objects/'+name+'.json', 'w') as file:\n",
    "        json.dump(obj, file)"
   ],
   "id": "2809d3f256e6a96",
   "outputs": [],
   "execution_count": 7
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-13T10:38:21.717441Z",
     "start_time": "2025-02-13T10:38:21.709006Z"
    }
   },
   "cell_type": "code",
   "source": "lexical_df = df.copy() # a df to compute lexical analysis on",
   "id": "bea695432b562638",
   "outputs": [],
   "execution_count": 8
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Sentence Splitting, Word Tokenization, Lemmatization, Stemming and POS Tagging\n",
    "We performed lexical analysis with Stanza, a collection of accurate and efficient tools for the linguistic analysis of many human languages.\n"
   ],
   "id": "ebc9c89c61b67f7c"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-13T10:38:27.531172Z",
     "start_time": "2025-02-13T10:38:23.752822Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# set the pipeline choosing English vocabulary and NLP functions to perform\n",
    "snlp = stanza.Pipeline(lang='en', processors='tokenize,mwt,pos,lemma')"
   ],
   "id": "a9ede4ce6879eb44",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-13 11:38:23 INFO: Checking for updates to resources.json in case models have been updated.  Note: this behavior can be turned off with download_method=None or download_method=DownloadMethod.REUSE_RESOURCES\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Downloading https://raw.githubusercontent.com/stanfordnlp/stanza-resources/main/resources_1.10.0.json:   0%|  â€¦"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "1bceb0965f3f4fcb9740580c5dcc7370"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-13 11:38:24 INFO: Downloaded file to C:\\Users\\claud\\stanza_resources\\resources.json\n",
      "2025-02-13 11:38:25 INFO: Loading these models for language: en (English):\n",
      "=================================\n",
      "| Processor | Package           |\n",
      "---------------------------------\n",
      "| tokenize  | combined          |\n",
      "| mwt       | combined          |\n",
      "| pos       | combined_charlm   |\n",
      "| lemma     | combined_nocharlm |\n",
      "=================================\n",
      "\n",
      "2025-02-13 11:38:25 INFO: Using device: cpu\n",
      "2025-02-13 11:38:25 INFO: Loading: tokenize\n",
      "2025-02-13 11:38:25 INFO: Loading: mwt\n",
      "2025-02-13 11:38:25 INFO: Loading: pos\n",
      "2025-02-13 11:38:26 INFO: Loading: lemma\n",
      "2025-02-13 11:38:27 INFO: Done loading processors!\n"
     ]
    }
   ],
   "execution_count": 9
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-13T10:38:43.584521Z",
     "start_time": "2025-02-13T10:38:42.383765Z"
    }
   },
   "cell_type": "code",
   "source": [
    "if check_existence('sentences'): # if the first obj exists for sure we created the others(?)\n",
    "    sentences = check_existence('sentences')\n",
    "    words = check_existence('words')\n",
    "    lemmatization = check_existence('lemmatization')\n",
    "    pos_tag = check_existence('pos_tag')\n",
    "\n",
    "else:\n",
    "    sentences = []\n",
    "    words = []\n",
    "    lemmatization = []\n",
    "    pos_tag = []\n",
    "\n",
    "    # iter on df records\n",
    "    for record in lexical_df[\"abstract\"]:\n",
    "        doc = snlp(record)\n",
    "        split_record = [[sent.text] for sent in doc.sentences] # sentence splitting\n",
    "        tokenized_record = []\n",
    "        lemmatized_record = []\n",
    "        pos_tag_record = []\n",
    "\n",
    "        # iter on each sentence of the record\n",
    "        for sentence in doc.sentences:\n",
    "            tokenized_sent = [(word.id, word.text) for word in sentence.tokens] # tokenization + id position\n",
    "            lemmatized_sent = [(word.text, word.lemma) if word.text != word.lemma else word.lemma for word in sentence.words] # lemmatization\n",
    "            pos_tag_sent = [(word.text, word.upos, word.xpos) for word in sentence.words] # POS tagging: universal POS tags and treebank-specific POS tags\n",
    "\n",
    "            tokenized_record.append(tokenized_sent)  # add tokenized sentence to list\n",
    "            lemmatized_record.append(lemmatized_sent) # add lemmatized sentence to list\n",
    "            pos_tag_record.append(pos_tag_sent) # add POS taggend sentence to list\n",
    "\n",
    "        sentences.append(split_record) # add record to list\n",
    "        words.append(tokenized_record) # add tokenized record to list\n",
    "        lemmatization.append(lemmatized_record) # add lemmatized record to list\n",
    "        pos_tag.append(pos_tag_record) # add POS tagged record to list\n",
    "\n",
    "    # Save each step to a file\n",
    "    save_step('sentences',sentences)\n",
    "    save_step('words',words)\n",
    "    save_step('lemmatization',lemmatization)\n",
    "    save_step('pos_tag',pos_tag)"
   ],
   "id": "874ff17201ff3ac4",
   "outputs": [],
   "execution_count": 10
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-13T10:38:47.591129Z",
     "start_time": "2025-02-13T10:38:47.578697Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# set each list as df columns to save progress\n",
    "lexical_df[\"sentences\"] = sentences\n",
    "lexical_df['words'] = words\n",
    "lexical_df['lemmatization'] = lemmatization\n",
    "lexical_df['pos_tag'] = pos_tag"
   ],
   "id": "c706a6e301bc897f",
   "outputs": [],
   "execution_count": 11
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-13T10:39:00.704576Z",
     "start_time": "2025-02-13T10:39:00.488221Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# display df\n",
    "lexical_df.head()"
   ],
   "id": "9aa75adcfcef1672",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "                  condition  \\\n",
       "0    cardiovascular disease   \n",
       "2  digestive system disease   \n",
       "4    cardiovascular disease   \n",
       "5                  neoplasm   \n",
       "8    nervous system disease   \n",
       "\n",
       "                                            abstract  \\\n",
       "0  Catheterization laboratory events and hospital...   \n",
       "2  Hyperplastic polyps seen at sigmoidoscopy are ...   \n",
       "4  Effect of local inhibition of gamma-aminobutyr...   \n",
       "5  Infection during chronic epidural catheterizat...   \n",
       "8  Multiple representations contribute to body kn...   \n",
       "\n",
       "                                           sentences  \\\n",
       "0  [[Catheterization laboratory events and hospit...   \n",
       "2  [[Hyperplastic polyps seen at sigmoidoscopy ar...   \n",
       "4  [[Effect of local inhibition of gamma-aminobut...   \n",
       "5  [[Infection during chronic epidural catheteriz...   \n",
       "8  [[Multiple representations contribute to body ...   \n",
       "\n",
       "                                               words  \\\n",
       "0  [[((1,), Catheterization), ((2,), laboratory),...   \n",
       "2  [[((1,), Hyperplastic), ((2,), polyps), ((3,),...   \n",
       "4  [[((1,), Effect), ((2,), of), ((3,), local), (...   \n",
       "5  [[((1,), Infection), ((2,), during), ((3,), ch...   \n",
       "8  [[((1,), Multiple), ((2,), representations), (...   \n",
       "\n",
       "                                       lemmatization  pos_tag  \n",
       "0  [[(Catheterization, catheterization), laborato...    False  \n",
       "2  [[(Hyperplastic, hyperplastic), (polyps, polyp...    False  \n",
       "4  [[(Effect, effect), of, local, inhibition, of,...    False  \n",
       "5  [[(Infection, infection), during, chronic, epi...    False  \n",
       "8  [[(Multiple, multiple), (representations, repr...    False  "
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>condition</th>\n",
       "      <th>abstract</th>\n",
       "      <th>sentences</th>\n",
       "      <th>words</th>\n",
       "      <th>lemmatization</th>\n",
       "      <th>pos_tag</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>cardiovascular disease</td>\n",
       "      <td>Catheterization laboratory events and hospital...</td>\n",
       "      <td>[[Catheterization laboratory events and hospit...</td>\n",
       "      <td>[[((1,), Catheterization), ((2,), laboratory),...</td>\n",
       "      <td>[[(Catheterization, catheterization), laborato...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>digestive system disease</td>\n",
       "      <td>Hyperplastic polyps seen at sigmoidoscopy are ...</td>\n",
       "      <td>[[Hyperplastic polyps seen at sigmoidoscopy ar...</td>\n",
       "      <td>[[((1,), Hyperplastic), ((2,), polyps), ((3,),...</td>\n",
       "      <td>[[(Hyperplastic, hyperplastic), (polyps, polyp...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>cardiovascular disease</td>\n",
       "      <td>Effect of local inhibition of gamma-aminobutyr...</td>\n",
       "      <td>[[Effect of local inhibition of gamma-aminobut...</td>\n",
       "      <td>[[((1,), Effect), ((2,), of), ((3,), local), (...</td>\n",
       "      <td>[[(Effect, effect), of, local, inhibition, of,...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>neoplasm</td>\n",
       "      <td>Infection during chronic epidural catheterizat...</td>\n",
       "      <td>[[Infection during chronic epidural catheteriz...</td>\n",
       "      <td>[[((1,), Infection), ((2,), during), ((3,), ch...</td>\n",
       "      <td>[[(Infection, infection), during, chronic, epi...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>nervous system disease</td>\n",
       "      <td>Multiple representations contribute to body kn...</td>\n",
       "      <td>[[Multiple representations contribute to body ...</td>\n",
       "      <td>[[((1,), Multiple), ((2,), representations), (...</td>\n",
       "      <td>[[(Multiple, multiple), (representations, repr...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 12
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-13T10:39:08.903648Z",
     "start_time": "2025-02-13T10:39:08.895177Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# an example of sentence splitting result for the first record\n",
    "sentences[0][0]"
   ],
   "id": "d696e0dc9ef201c2",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Catheterization laboratory events and hospital outcome with direct angioplasty for acute myocardial infarction To assess the safety of direct infarct angioplasty without antecedent thrombolytic therapy, catheterization laboratory and hospital events were assessed in consecutively treated patients with infarctions involving the left anterior descending (n = 100 patients), right (n = 100), and circumflex (n = 50) coronary arteries.']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 13
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Stop-words Removal\n",
    "Stop-words are common words that do not carry specific meaning, such as articles, prepositions, and conjunctions.\n",
    "It is usually performed after lexical analysis to avoid inaccuracies in subsequent syntactic or semantic analyses."
   ],
   "id": "a182d9f1d2bec548"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "nltk.download('stopwords')",
   "id": "656c43531f3b89ac"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "if check_existence('stopwords_removal'):\n",
    "    stopwords_removal = check_existence('stopwords_removal')\n",
    "\n",
    "else:\n",
    "    stopwords = nltk.corpus.stopwords.words('english')\n",
    "\n",
    "    stopwords_removal = []\n",
    "\n",
    "    for record in lexical_df['sentences']:\n",
    "        filtered_record = []\n",
    "        for sentence in record:\n",
    "            filtered_sentence = [word for word in sentence if word.lower() not in stopwords]\n",
    "            filtered_record.append(filtered_sentence)\n",
    "\n",
    "        stopwords_removal.append(filtered_record)\n",
    "\n",
    "    save_step('stopwords_removal',stopwords_removal)"
   ],
   "id": "878fed37d22a5c10"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "lexical_df[\"stopwords_removal\"] = stopwords_removal",
   "id": "5bb63519de814d6b"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "lexical_df.head()",
   "id": "78456432d8cd9bca"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Syntax Analysis\n",
    "Syntax analysis consists of:\n",
    "- Shallow Parsing\n",
    "- Deep Parsing"
   ],
   "id": "50cc3c5fd5c9b2be"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Shallow Parsing\n",
    "Syntactic parsing extends chunking by generating a parse tree. This tree organizes POS-tagging results as leaf nodes and syntactic structures (often chunks) as intermediate nodes, connected hierarchically without representing specific relationships.\n"
   ],
   "id": "f4c67200ae07ed99"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "syntax_df = df.copy() # a df to compute syntax analysis on",
   "id": "792118acf8856410"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "if check_existence('chunking'):\n",
    "    chunking = check_existence('chunking')\n",
    "\n",
    "else:\n",
    "    # Define the grammar and the chunk parser\n",
    "    grammar = \"NP: {<NNP><NNP>}\"\n",
    "    cp = nltk.RegexpParser(grammar) # chunk parser\n",
    "\n",
    "    # Apply chunking to each record\n",
    "    chunking = []\n",
    "    for record in lexical_df['pos_tag']:\n",
    "        chunked_record = [cp.parse(sentence) for sentence in record]\n",
    "\n",
    "        chunking.append(chunked_record)\n",
    "\n",
    "    save_step('chunking',chunking)"
   ],
   "id": "237ec23b70378d38"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Save the chunking results into the dataframe\n",
    "syntax_df[\"shallow_parsing\"] = chunking"
   ],
   "id": "925284a7b46c5c8c"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Display the dataframe\n",
    "syntax_df.head()"
   ],
   "id": "6ed89b847d8f1594"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Display the chunking result for the first record\n",
    "print(syntax_df['shallow_parsing'][0][4])\n",
    "#syntax_df['chunking'][0][4]"
   ],
   "id": "2a6b0016a5fa4b80"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# To draw the parse tree\n",
    "#syntax_df['chunking'][1][1].draw()"
   ],
   "id": "7726ddbb755cc2ee"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Deep Parsing\n",
    "Differently from _Shallow parsing_, _Deep parsing_ aims to infer dependency relationships between nodes.\n",
    "The result is a dependency graph which relates words that are syntactically linked."
   ],
   "id": "2745fef0ded9a53b"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Load the SpaCy model\n",
    "nlp = spacy.load(\"en_core_web_sm\")"
   ],
   "id": "57b89d7fca319004"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "if check_existence('deep_parsing'):\n",
    "    deep_parsing = check_existence('deep_parsing')\n",
    "\n",
    "else:\n",
    "    deep_parsing = []\n",
    "    for sentences in syntax_df[\"abstract\"]:\n",
    "        sentence_dep = []\n",
    "        doc = nlp(sentences)\n",
    "        for token in doc:\n",
    "            sentence_dep.append((str(token.text), str(token.dep_), str(token.head.text), str([child for child in token.children])))\n",
    "            # creates a tuple containing the token, dependency nature, head and all dependents of the token\n",
    "        deep_parsing.append(sentence_dep)\n",
    "\n",
    "    save_step('deep_parsing',deep_parsing)"
   ],
   "id": "f1a810995af0fcaf"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "syntax_df[\"deep_parsing\"] = deep_parsing",
   "id": "e1e485ae7f9b9845"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "syntax_df.head()",
   "id": "33d2d0ced09eae33"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Seleziona la prima riga del dataframe\n",
    "sentence = syntax_df[\"abstract\"].iloc[0]\n",
    "\n",
    "# Analizza la frase\n",
    "doc = nlp(sentence)\n",
    "\n",
    "# Visualizza il grafico di dipendenze\n",
    "displacy.render(doc, style=\"dep\", jupyter=True)"
   ],
   "id": "710d04fc5a75bc52"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Semantic Analysis\n",
    "\n",
    "Semantic analysis aims to extract the meaning of a text, focusing on the relationships between entities and the context in which they appear."
   ],
   "id": "3bbd759b9627f213"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Entity Extraction\n",
    "\n",
    "Entity extraction identifies named entities in a text, such as people, organizations, or locations."
   ],
   "id": "78ec0f242884c47a"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "semantic_df = df.copy() # a df to compute semantic analysis on",
   "id": "48d674fbb703e9c6"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "This code extracts named entities from the \"abstract\" column of a DataFrame called syntax_df using a NLP model.\n",
    "For each record, it collects the entities and their labels, storing them in a list of tuples."
   ],
   "id": "cb89044994000740"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "pipe = pipeline(\"token-classification\", model=\"Clinical-AI-Apollo/Medical-NER\", aggregation_strategy='simple')",
   "id": "46358767f3e1e7af"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# if the entities are already extracted, load them from the pickle file\n",
    "if check_existence('entities'):\n",
    "    entities = check_existence('entities')\n",
    "\n",
    "else:\n",
    "    entities = []\n",
    "    for record in semantic_df[\"abstract\"]:\n",
    "        result = pipe(record)\n",
    "\n",
    "        record_entities = [(entity['word'], entity['entity_group']) for entity in result]\n",
    "\n",
    "        entities.append(record_entities)\n",
    "\n",
    "    save_step('entities',entities)"
   ],
   "id": "ba1f95a2d1970070"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "semantic_df[\"entities\"] = entities",
   "id": "eaad5b5314935434"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "To get a sense of the entities extracted from the text, we can display the unique entities found in the dataset.",
   "id": "7a581a198a96cbd0"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Flatten the list of entities and extract only the labels\n",
    "all_labels = pd.Series([label for record in semantic_df[\"entities\"] for _, label in record])\n",
    "\n",
    "# Perform value counts on the labels\n",
    "all_labels.value_counts()"
   ],
   "id": "ee14f5ef879a2473"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "semantic_df.head()",
   "id": "322e01f87163f9a1"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "def display_entities(pipe, document):\n",
    "    \"\"\"\n",
    "    Display recognized entities using displacy.render().\n",
    "\n",
    "    Parameters:\n",
    "        pipe: Hugging Face pipeline for entity recognition.\n",
    "        document (str): Text to process.\n",
    "\n",
    "    Returns:\n",
    "        - Visualization of entities using displacy.\n",
    "        - List of entities and their labels.\n",
    "    \"\"\"\n",
    "    # Run the pipeline on the document\n",
    "    result = pipe(document)\n",
    "\n",
    "    # Create a blank SpaCy model to handle the document\n",
    "    nlp = spacy.blank(\"en\")\n",
    "    doc = nlp.make_doc(document)\n",
    "\n",
    "    # Add entities manually\n",
    "    ents = []\n",
    "    for entity in result:\n",
    "        start_char = entity['start']\n",
    "        end_char = entity['end']\n",
    "        label = entity['entity_group']\n",
    "        # Create a span manually\n",
    "        span = doc.char_span(start_char, end_char, label=label, alignment_mode=\"expand\")\n",
    "        if span is not None:\n",
    "            ents.append(span)\n",
    "\n",
    "    # Ensure no overlapping spans\n",
    "    ents = spacy.util.filter_spans(ents)\n",
    "\n",
    "    # Assign the entities to the document\n",
    "    doc.ents = ents\n",
    "\n",
    "    # Display with displacy\n",
    "    displacy.render(doc, style='ent', jupyter=True)"
   ],
   "id": "f263b89a12c5c3a1"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "display_entities(pipe, semantic_df[\"abstract\"].iloc[2])",
   "id": "11e5180fdcb11adb"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "semantic_df.head()",
   "id": "6fcf59f2313baa"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Filter and display only the tuples where the label is BIOLOGICAL_STRUCTURE\n",
    "semantic_df['entities'].apply(lambda x: [(text, label) for text, label in x if label == 'MEDICATION'])"
   ],
   "id": "6fea92e123c48a83"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Relation Extraction\n",
    "\n",
    "Relation extraction identifies the connections between entities in a text, such as the subject, object, and verb of a sentence.\n",
    "\n",
    "This process is more complex than entity extraction, as it requires understanding the syntactic structure of the text to infer relationships between entities.\n",
    "\n",
    "1. **Iterate through sentences**:\n",
    "   Process each sentence (`doc`) and its entities, storing entities in a set for faster lookup (`entities_set`).\n",
    "\n",
    "2. **Extract relations from tokens**:\n",
    "   Loop through tokens with dependencies like \"ROOT\" or \"VERB\" to identify:\n",
    "   - **Subjects**: Found in children with dependencies like \"nsubj\" or \"agent.\"\n",
    "   - **Objects**: Found in children with dependencies like \"dobj\" or \"pobj.\"\n",
    "\n",
    "3. **Record relations**:\n",
    "   - Direct relations: `(subject, verb, object).`\n",
    "   - Prepositional relations: Handle `prep` and `pobj` to form `(subj, verb_prep, obj)` or similar.\n",
    "\n",
    "4. **Handle conjunctions**:\n",
    "   Add relations involving conjunctive tokens (`conj`).\n",
    "\n"
   ],
   "id": "3b9689cc36b0ec0c"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### TF_IDF",
   "id": "b1621a8dc77c3727"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "from sklearn.feature_extraction.text import TfidfVectorizer",
   "id": "ebea5529baee7da0"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "The following vectorizer takes a sequence of byte strings, converts it into lowercase, extracts unigrams, and calculates a TF-IDF score. It contains stop words from english vocabulary, the n-grams that occur in more than 60% of documents or in less than 10% of documents will be ignored.",
   "id": "5b924f1987d3194d"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# build vectorizer\n",
    "vectorizer = TfidfVectorizer(input='content', use_idf=True, lowercase=True,\n",
    "analyzer='word', ngram_range=(1, 1), stop_words='english', vocabulary=None, min_df=0.10, max_df=0.60)"
   ],
   "id": "8b8209a4a7a3925a"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# build TF-IDF matrix\n",
    "tfidf_matrix = vectorizer.fit_transform(df['abstract'].values)\n",
    "print(f\"Matrix dimension: {tfidf_matrix.shape}\")"
   ],
   "id": "9bb2413892d07bb5"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# extract features and set results into a df\n",
    "feature_names = vectorizer.get_feature_names_out()\n",
    "tfidf_df = pd.DataFrame(tfidf_matrix.toarray(), index=df.index, columns=feature_names)"
   ],
   "id": "cd81c9687326db51"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# display df\n",
    "tfidf_df.head()"
   ],
   "id": "e0832287400659db"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# show each feature frequency\n",
    "tfidf_df.loc['00_Document Frequency'] = (tfidf_df > 0).sum()"
   ],
   "id": "cc8f8ed7ddaab151"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "tfidf_df.tail()",
   "id": "31dc6a3357bdd86b"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## DATABASE",
   "id": "db19cbab5d16363f"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "This is an image of the database schema we are going to create.\n",
    "\n",
    "<div style=\"text-align: center;\">\n",
    "    <img src=\"images/er_schema.png\" alt=\"Database Schema\" width=\"700\"/>\n",
    "</div>"
   ],
   "id": "c361b5d329b4b9e2"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Data Definition Language",
   "id": "7413f5171a7e7561"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "CREATE_MEDICATIONS = \"\"\"\n",
    "DROP TABLE IF EXISTS MEDICATIONS;\n",
    "\n",
    "CREATE TABLE MEDICATIONS(\n",
    "id SERIAL PRIMARY KEY,\n",
    "name VARCHAR NOT NULL,\n",
    "dosage VARCHAR\n",
    ");\n",
    "\"\"\"\n",
    "\n",
    "# Save to file\n",
    "with open('database/ddl/create_medications.sql', 'w') as file:\n",
    "    file.write(CREATE_MEDICATIONS)"
   ],
   "id": "57aab2118eb14c37"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "CREATE_DISEASES = \"\"\"\n",
    "DROP TABLE IF EXISTS DISEASES;\n",
    "\n",
    "CREATE TABLE DISEASES(\n",
    "id SERIAL PRIMARY KEY,\n",
    "name VARCHAR NOT NULL,\n",
    "condition VARCHAR NOT NULL\n",
    ");\n",
    "\"\"\"\n",
    "\n",
    "# Save to file\n",
    "with open('database/ddl/create_diseases.sql', 'w') as file:\n",
    "    file.write(CREATE_DISEASES)"
   ],
   "id": "7bc5693665d47ed9"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "CREATE_SYMPTOMS = \"\"\"\n",
    "DROP TABLE IF EXISTS SYMPTOMS;\n",
    "\n",
    "CREATE TABLE SYMPTOMS(\n",
    "id SERIAL PRIMARY KEY,\n",
    "name VARCHAR NOT NULL\n",
    ");\n",
    "\"\"\"\n",
    "\n",
    "# Save to file\n",
    "with open('database/ddl/create_symptoms.sql', 'w') as file:\n",
    "    file.write(CREATE_SYMPTOMS)"
   ],
   "id": "cf2b00f6a918cbc1"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "CREATE_DIAGNOSTIC_TESTS = \"\"\"\n",
    "DROP TABLE IF EXISTS DIAGNOSTIC_TESTS;\n",
    "\n",
    "CREATE TABLE DIAGNOSTIC_TESTS(\n",
    "id SERIAL PRIMARY KEY,\n",
    "name VARCHAR NOT NULL\n",
    ");\n",
    "\"\"\"\n",
    "\n",
    "# Save to file\n",
    "with open('database/ddl/create_diagnostic_tests.sql', 'w') as file:\n",
    "    file.write(CREATE_DIAGNOSTIC_TESTS)"
   ],
   "id": "e9898293678b6393"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "CREATE_BIOLOGICAL_STRUCTURES = \"\"\"\n",
    "DROP TABLE IF EXISTS BIOLOGICAL_STRUCTURES;\n",
    "\n",
    "CREATE TABLE BIOLOGICAL_STRUCTURES(\n",
    "id SERIAL PRIMARY KEY,\n",
    "name VARCHAR NOT NULL\n",
    ");\n",
    "\"\"\"\n",
    "\n",
    "# Save to file\n",
    "with open('database/ddl/create_biological_structures.sql', 'w') as file:\n",
    "    file.write(CREATE_BIOLOGICAL_STRUCTURES)"
   ],
   "id": "7a34b954fd1d21db"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "CREATE_TREATMENTS = \"\"\"\n",
    "DROP TABLE IF EXISTS TREATMENTS;\n",
    "\n",
    "CREATE TABLE TREATMENTS(\n",
    "id SERIAL PRIMARY KEY,\n",
    "disease_id INT NOT NULL,\n",
    "treatment_id INT NOT NULL,\n",
    "FOREIGN KEY (disease_ID) REFERENCES DISEASES(id),\n",
    "FOREIGN KEY (treatment_ID) REFERENCES TREATMENTS(id)\n",
    ");\n",
    "\"\"\"\n",
    "\n",
    "# Save to file\n",
    "with open('database/ddl/create_treatments.sql', 'w') as file:\n",
    "    file.write(CREATE_TREATMENTS)"
   ],
   "id": "7569ec63e997cc84"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "CREATE_MANIFESTATIONS = \"\"\"\n",
    "DROP TABLE IF EXISTS MANIFESTATIONS;\n",
    "\n",
    "CREATE TABLE MANIFESTATIONS(\n",
    "id SERIAL PRIMARY KEY,\n",
    "disease_id INT NOT NULL,\n",
    "symptom_id INT NOT NULL,\n",
    "severity VARCHAR,\n",
    "FOREIGN KEY (disease_id) REFERENCES DISEASES(id),\n",
    "FOREIGN KEY (symptom_id) REFERENCES SYMPTOMS(id)\n",
    ");\n",
    "\"\"\"\n",
    "\n",
    "# Save to file\n",
    "with open('database/ddl/create_manifestations.sql', 'w') as file:\n",
    "    file.write(CREATE_MANIFESTATIONS)"
   ],
   "id": "15e0443f18dc6d74"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "CREATE_DIAGNOSIS = \"\"\"\n",
    "DROP TABLE IF EXISTS DIAGNOSIS;\n",
    "\n",
    "CREATE TABLE DIAGNOSIS(\n",
    "id SERIAL PRIMARY KEY,\n",
    "disease_id INT NOT NULL,\n",
    "diagnostic_test_id INT NOT NULL,\n",
    "FOREIGN KEY (disease_id) REFERENCES DISEASES(id),\n",
    "FOREIGN KEY (diagnostic_test_id) REFERENCES DIAGNOSTIC_TESTS(id)\n",
    ");\"\"\"\n",
    "\n",
    "# Save to file\n",
    "with open('database/ddl/create_diagnosis.sql', 'w') as file:\n",
    "    file.write(CREATE_DIAGNOSIS)"
   ],
   "id": "6ce4ff5b1356bc43"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "CREATE_INVOLVEMENTS = \"\"\"\n",
    "DROP TABLE IF EXISTS INVOLVEMENTS;\n",
    "\n",
    "CREATE TABLE INVOLVEMENTS(\n",
    "id SERIAL PRIMARY KEY,\n",
    "diseaseID INT NOT NULL,\n",
    "biologicalStrID INT NOT NULL,\n",
    "FOREIGN KEY (diseaseID) REFERENCES DISEASES(id),\n",
    "FOREIGN KEY (biologicalStrID) REFERENCES BIOLOGICAL_STRUCTURES(id)\n",
    ");\n",
    "\"\"\"\n",
    "\n",
    "# Save to file\n",
    "with open('database/ddl/create_involvements.sql', 'w') as file:\n",
    "    file.write(CREATE_INVOLVEMENTS)"
   ],
   "id": "6d0f1232a920eb4f"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# List of DDL files to concatenate\n",
    "ddl_files = [\n",
    "    'database/ddl/create_medications.sql',\n",
    "    'database/ddl/create_diseases.sql',\n",
    "    'database/ddl/create_symptoms.sql',\n",
    "    'database/ddl/create_diagnostic_tests.sql',\n",
    "    'database/ddl/create_biological_structures.sql',\n",
    "    'database/ddl/create_treatments.sql',\n",
    "    'database/ddl/create_manifestations.sql',\n",
    "    'database/ddl/create_diagnosis.sql',\n",
    "    'database/ddl/create_involvements.sql'\n",
    "]\n",
    "\n",
    "# Output file\n",
    "output_file = 'database/ddl/create_all_tables.sql'\n",
    "\n",
    "# Concatenate the contents of all DDL files into the output file\n",
    "with open(output_file, 'w') as outfile:\n",
    "    for ddl_file in ddl_files:\n",
    "        with open(ddl_file, 'r') as infile:\n",
    "            outfile.write(infile.read())"
   ],
   "id": "a549566a6b8ca462"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Data Manipulation Language",
   "id": "d4215f926f0cb597"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Symptom entities\n",
    "symptoms = []\n",
    "\n",
    "for entities in semantic_df['entities']:\n",
    "    for text, label in entities:\n",
    "        if label == 'SIGN_SYMPTOM':\n",
    "            symptoms.append(text)\n",
    "\n",
    "symptoms"
   ],
   "id": "cea7538f125c51ef"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Normalize to lowercase before creating the set\n",
    "symptoms = list(set(s.lower() for s in symptoms))"
   ],
   "id": "dfea82e40367ba0d"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Insert the symptoms into the SYMPTOMS table\n",
    "insert_symptoms = []\n",
    "\n",
    "for symptom in symptoms:\n",
    "    query = f\"INSERT INTO SYMPTOMS (name) VALUES ('{symptom}');\"\n",
    "    insert_symptoms.append(query)\n",
    "\n",
    "# Save the queries to a file\n",
    "with open('database/dml/insert_symptoms.sql', 'w') as file:\n",
    "    for query in insert_symptoms:\n",
    "        file.write(query + '\\n')"
   ],
   "id": "eeb887fb38edb83b"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# List to store (condition, disease) tuples\n",
    "diseases = []\n",
    "\n",
    "# Iterate through the rows of the DataFrame\n",
    "for condition, entities in zip(semantic_df['condition'], semantic_df['entities']):\n",
    "    for text, label in entities:\n",
    "        if label == 'DISEASE_DISORDER':\n",
    "            diseases.append((condition, text))\n",
    "\n",
    "len(diseases)"
   ],
   "id": "64b1b32bfcdbc635"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Remove duplicates in a case-insensitive way\n",
    "unique_diseases = list({(condition, disease.lower()) for condition, disease in diseases})\n",
    "\n",
    "len(unique_diseases)"
   ],
   "id": "c74d117363b9059f"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "insert_diseases = []\n",
    "\n",
    "for condition, disease in unique_diseases:\n",
    "    disease_escaped = disease.replace(\"'\", \"''\")\n",
    "\n",
    "    query = f\"INSERT INTO DISEASES (condition, name) VALUES ('{condition}', '{disease_escaped}');\"\n",
    "    insert_diseases.append(query)\n",
    "\n",
    "# Save the queries to a file\n",
    "with open('database/dml/insert_diseases.sql', 'w') as file:\n",
    "    for query in insert_diseases:\n",
    "        file.write(query + '\\n')\n"
   ],
   "id": "67e4e87fc725ec75"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Diagnostic Test entities\n",
    "diagnostic_tests = []\n",
    "\n",
    "for entities in semantic_df['entities']:\n",
    "    for text, label in entities:\n",
    "        if label == 'DIAGNOSTIC_PROCEDURE':\n",
    "            diagnostic_tests.append(text)\n",
    "\n",
    "diagnostic_tests"
   ],
   "id": "6071e4157663c01b"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Normalize to lowercase before creating the set\n",
    "diagnostic_tests = list(set(d.lower() for d in diagnostic_tests))"
   ],
   "id": "93b3ddf130482122"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "insert_diagnostic_tests = []\n",
    "\n",
    "for diagnostic_test in diagnostic_tests:\n",
    "    # escape single quotes\n",
    "    diagnostic_test = diagnostic_test.replace(\"'\", \"''\")\n",
    "\n",
    "    query = f\"INSERT INTO DIAGNOSTIC_TESTS (name) VALUES ('{diagnostic_test}');\"\n",
    "    insert_diagnostic_tests.append(query)\n",
    "\n",
    "with open('database/dml/insert_diagnostic_tests.sql', 'w') as file:\n",
    "    for query in insert_diagnostic_tests:\n",
    "        file.write(query + '\\n')"
   ],
   "id": "da5806b946c8dfa6"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Biological Structure entities\n",
    "biological_structures = []\n",
    "\n",
    "for entities in semantic_df['entities']:\n",
    "    for text, label in entities:\n",
    "        if label == 'BIOLOGICAL_STRUCTURE':\n",
    "            biological_structures.append(text)\n",
    "\n",
    "biological_structures"
   ],
   "id": "d3be34b7a6e2b229"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Normalize to lowercase before creating the set\n",
    "biological_structures = list(set(b.lower() for b in biological_structures))"
   ],
   "id": "aa70463279b8c7d8"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "insert_biological_structures = []\n",
    "\n",
    "for biological_structure in biological_structures:\n",
    "    # escape single quotes\n",
    "    biological_structure = biological_structure.replace(\"'\", \"''\")\n",
    "    query = f\"INSERT INTO BIOLOGICAL_STRUCTURES (name) VALUES ('{biological_structure}');\"\n",
    "    insert_biological_structures.append(query)\n",
    "\n",
    "with open('database/dml/insert_biological_structures.sql', 'w') as file:\n",
    "    for query in insert_biological_structures:\n",
    "        file.write(query + '\\n')"
   ],
   "id": "c860e0f7d24304bd"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Query Language",
   "id": "57774485c52ddfca"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "semantic_df[semantic_df['entities'].apply(\n",
    "    lambda entities: any(label == 'BIOLOGICAL_STRUCTURE' for _, label in entities)\n",
    ")]"
   ],
   "id": "f87c50ca671bfa8e"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "display_entities(pipe, semantic_df[\"abstract\"].iloc[3])",
   "id": "317310639b2340a7"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# FINALE FINALE GIURO (senza lemma)",
   "id": "9e8726f45d085147"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Initialize the NLP pipeline\n",
    "nlp = stanza.Pipeline(lang='en', processors='tokenize,mwt,pos,lemma,depparse')\n",
    "\n",
    "def find_nearest(anchor_text, candidates, words):\n",
    "    \"\"\"Finds the nearest candidate word to the anchor_text in a sentence.\"\"\"\n",
    "    if not candidates:\n",
    "        return None  # No candidates available\n",
    "\n",
    "    # Find closest match by iterating over words\n",
    "    anchor_idx = None\n",
    "    for idx, word in enumerate(words):\n",
    "        if anchor_text.lower() in word.lower():\n",
    "            anchor_idx = idx\n",
    "            break\n",
    "\n",
    "    if anchor_idx is None:\n",
    "        return None  # Anchor word not found\n",
    "\n",
    "    # Find nearest candidate\n",
    "    nearest_candidate = None\n",
    "    min_distance = float('inf')\n",
    "    for cand in candidates:\n",
    "        for idx, word in enumerate(words):\n",
    "            if cand.lower() in word.lower():\n",
    "                distance = abs(idx - anchor_idx)\n",
    "                if distance < min_distance:\n",
    "                    min_distance = distance\n",
    "                    nearest_candidate = cand\n",
    "    return nearest_candidate\n",
    "\n",
    "def extract_relations_generic(text, entities, condition, rules):\n",
    "    \"\"\"Extracts relations from text using a list of rules.\"\"\"\n",
    "    results = {rule['relation_type']: [] for rule in rules}\n",
    "\n",
    "    doc = nlp(text)\n",
    "    for sentence in doc.sentences:\n",
    "        words = [w.text for w in sentence.words]\n",
    "\n",
    "        for rule in rules:\n",
    "            role_candidates = {}\n",
    "            for role_name, spec in rule[\"roles\"].items():\n",
    "                if role_name == \"condition\":\n",
    "                    role_candidates[role_name] = [condition]  # Use actual condition value\n",
    "                else:\n",
    "                    role_candidates[role_name] = [\n",
    "                        ent_text for ent_text, ent_label in entities\n",
    "                        if ent_label == spec[\"label\"] and ent_text in words\n",
    "                    ]\n",
    "\n",
    "            # Skip rule if any required role is missing\n",
    "            if any(len(role_candidates[role]) == 0 for role, spec in rule[\"roles\"].items() if not spec.get(\"optional\", False)):\n",
    "                continue\n",
    "\n",
    "            # Process roles with selection functions\n",
    "            fixed_roles = [role for role, spec in rule[\"roles\"].items() if \"select_func\" not in spec]\n",
    "            fixed_candidate_lists = [role_candidates[role] if role_candidates[role] else [None] for role in fixed_roles]\n",
    "\n",
    "            for combo in product(*fixed_candidate_lists):\n",
    "                relation = {role: combo[idx] for idx, role in enumerate(fixed_roles)}\n",
    "\n",
    "                for role, spec in rule[\"roles\"].items():\n",
    "                    if \"select_func\" in spec:\n",
    "                        anchor_role = spec[\"anchor\"]\n",
    "                        anchor_val = relation.get(anchor_role)\n",
    "                        candidate_list = role_candidates.get(role, [])\n",
    "                        relation[role] = spec[\"select_func\"](anchor_val, candidate_list, words)\n",
    "\n",
    "                results[rule[\"relation_type\"]].append(relation)\n",
    "\n",
    "    return results\n",
    "\n",
    "# ----------------------------------------------\n",
    "# Updated rules, ensuring `condition` is not constant but dynamically set.\n",
    "\n",
    "rules = [\n",
    "    {\n",
    "        \"relation_type\": \"med_disease\",\n",
    "        \"roles\": {\n",
    "            \"medication\": {\"label\": \"MEDICATION\"},\n",
    "            \"dosage\": {\"label\": \"DOSAGE\", \"optional\": True, \"select_func\": find_nearest, \"anchor\": \"medication\"},\n",
    "            \"disease\": {\"label\": \"DISEASE_DISORDER\"},\n",
    "            \"condition\": {}  # Now dynamically set\n",
    "        }\n",
    "    },\n",
    "#    {\n",
    "#        \"relation_type\": \"disease_manifestation_symptom\",\n",
    "#        \"roles\": {\n",
    "#            \"disease\": {\"label\": \"DISEASE_DISORDER\"},\n",
    "#            \"symptom\": {\"label\": \"SIGN_SYMPTOM\"},\n",
    "#            \"severity\": {\n",
    "#                \"label\": \"SEVERITY\",\n",
    "#                \"optional\": True,\n",
    "#                \"select_func\": lambda anchor, candidates, words: find_nearest(anchor, candidates, words) if candidates else None,\n",
    "#                \"anchor\": \"symptom\"\n",
    "#            },\n",
    "#            \"condition\": {}\n",
    "#        }\n",
    "#    },\n",
    "    {\n",
    "        \"relation_type\": \"bio_disease\",\n",
    "        \"roles\": {\n",
    "            \"biological_structure\": {\"label\": \"BIOLOGICAL_STRUCTURE\"},\n",
    "            \"disease\": {\"label\": \"DISEASE_DISORDER\"},\n",
    "            \"condition\": {}\n",
    "        }\n",
    "    }\n",
    "#    {\n",
    "#        \"relation_type\": \"diagnosis\",\n",
    "#        \"roles\": {\n",
    "#            \"disease\": {\"label\": \"DISEASE_DISORDER\"},\n",
    "#            \"diagnostic_test\": {\"label\": \"DIAGNOSTIC_PROCEDURE\"},\n",
    "#            \"condition\": {}\n",
    "#        }\n",
    "#    }\n",
    "]\n",
    "\n",
    "# ----------------------------------------------\n",
    "\n",
    "all_results = {rule[\"relation_type\"]: [] for rule in rules}\n",
    "\n",
    "\n",
    "for idx, row in semantic_df.iterrows():\n",
    "    text = row['abstract']\n",
    "    entities = row['entities']\n",
    "    condition_value = row['condition']  # Ensure correct condition value\n",
    "\n",
    "    row_results = extract_relations_generic(text, entities, condition_value, rules)\n",
    "\n",
    "    for rel_type in all_results:\n",
    "        all_results[rel_type].extend(row_results.get(rel_type, []))"
   ],
   "id": "3bd4acbe4803534f"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# 1. Deduplicate symptoms.\n",
    "unique_symptoms = set()\n",
    "# 2. Deduplicate manifestations.\n",
    "# Each manifestation record is a tuple:\n",
    "# (disease_name, disease_condition, symptom_name, severity)\n",
    "unique_manifestations = set()\n",
    "\n",
    "for rel in all_results[\"disease_manifestation_symptom\"]:\n",
    "    # Handle disease, symptom, and manifestation extraction safely\n",
    "    disease = rel.get(\"disease\", \"\")\n",
    "    symptom = rel.get(\"symptom\", \"\")\n",
    "    manifestation = rel.get(\"manifestation\", {})\n",
    "\n",
    "    disease_condition = rel.get(\"condition\", \"\").strip().lower() if \"condition\" in rel else None\n",
    "\n",
    "    disease_name = disease.strip().lower()\n",
    "\n",
    "    symptom_name = symptom.strip().lower()\n",
    "\n",
    "    #  Extract severity correctly\n",
    "    if isinstance(rel.get(\"severity\"), str) and rel[\"severity\"].strip():\n",
    "        severity = rel[\"severity\"].strip().lower()\n",
    "    else:\n",
    "        severity = None\n",
    "\n",
    "    unique_symptoms.add(symptom_name)\n",
    "    unique_manifestations.add((disease_name, disease_condition, symptom_name, severity))\n",
    "\n",
    "# --- Generate INSERT queries for SYMPTOMS ---\n",
    "symptom_inserts = []\n",
    "for symptom_name in unique_symptoms:\n",
    "    symptom_name = symptom_name.replace(\"'\", \"''\")\n",
    "    query = f\"INSERT INTO SYMPTOMS (name) VALUES ('{symptom_name}');\"\n",
    "    symptom_inserts.append(query)\n",
    "\n",
    "# --- Generate INSERT queries for MANIFESTATIONS ---\n",
    "manifestation_inserts = []\n",
    "for disease_name, disease_condition, symptom_name, severity in unique_manifestations:\n",
    "    severity_value = f\"'{severity}'\" if severity is not None else \"NULL\"\n",
    "\n",
    "    disease_name = disease_name.replace(\"'\", \"''\")\n",
    "    symptom_name = symptom_name.replace(\"'\", \"''\")\n",
    "    severity_value = severity_value.replace(\"'\", \"''\")\n",
    "\n",
    "    # Disease subquery with condition and name\n",
    "    disease_subquery = f\"(SELECT id FROM DISEASES WHERE name = '{disease_name}' AND condition = '{disease_condition}')\"\n",
    "    symptom_subquery = f\"(SELECT id FROM SYMPTOMS WHERE name = '{symptom_name}')\"\n",
    "\n",
    "    query = (f\"INSERT INTO MANIFESTATIONS (severity, disease_id, symptom_id) VALUES (\\n\"\n",
    "             f\"    {severity_value},\\n\"\n",
    "             f\"    {disease_subquery},\\n\"\n",
    "             f\"    {symptom_subquery}\\n\"\n",
    "             f\");\")\n",
    "    manifestation_inserts.append(query)\n",
    "\n",
    "# --- Save the INSERT queries to files ---\n",
    "\n",
    "# Save Symptom INSERT queries\n",
    "with open('database/dml/insert_symptoms.sql', 'w') as sym_file:\n",
    "    for query in symptom_inserts:\n",
    "        sym_file.write(query + \"\\n\")\n",
    "\n",
    "# Save Manifestation INSERT queries\n",
    "with open('database/dml/insert_manifestations.sql', 'w') as man_file:\n",
    "    for query in manifestation_inserts:\n",
    "        man_file.write(query + \"\\n\")"
   ],
   "id": "caef8aa0c1f9978f"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# --- Deduplication for generating INSERT queries ---\n",
    "unique_medications = set()\n",
    "unique_treatments = set()\n",
    "\n",
    "for triple in all_results[\"med_disease\"]:\n",
    "    med_name = triple.get(\"medication\", \"\").strip().lower()\n",
    "    med_dosage = triple.get(\"dosage\", \"\").strip().lower() if triple.get(\"dosage\") else None\n",
    "\n",
    "    # Add the medication as a tuple (name, dosage)\n",
    "    unique_medications.add((med_name, med_dosage))\n",
    "\n",
    "    disease_name = triple.get(\"disease\", \"\").strip().lower()\n",
    "    disease_condition = triple.get(\"condition\", \"\").strip().lower() if \"condition\" in triple else None\n",
    "\n",
    "    # The treatment record is now a tuple: (medication name, medication dosage, disease name, disease condition)\n",
    "    unique_treatments.add((med_name, med_dosage, disease_name, disease_condition))\n",
    "\n",
    "# --- Generate INSERT queries for Medications ---\n",
    "medication_inserts = []\n",
    "for med_name, med_dosage in unique_medications:\n",
    "    dosage_value = f\"'{med_dosage}'\" if med_dosage is not None else \"NULL\"\n",
    "\n",
    "    med_name = med_name.replace(\"'\", \"''\")\n",
    "    dosage_value = dosage_value.replace(\"'\", \"''\")\n",
    "\n",
    "    query = f\"INSERT INTO MEDICATIONS (name, dosage) VALUES ('{med_name}', '{dosage_value}');\"\n",
    "\n",
    "    medication_inserts.append(query)\n",
    "\n",
    "# --- Generate INSERT queries for Treatments ---\n",
    "treatment_inserts = []\n",
    "for med_name, med_dosage, disease_name, disease_condition in unique_treatments:\n",
    "    # Medication subquery:\n",
    "    # escape single quotes\n",
    "    med_name = med_name.replace(\"'\", \"''\")\n",
    "    if med_dosage is not None:\n",
    "        med_dosage = med_dosage.replace(\"'\", \"''\")\n",
    "        med_subquery = f\"(SELECT id FROM MEDICATIONS WHERE name = '{med_name}' AND dosage = '{med_dosage}')\"\n",
    "    else:\n",
    "        med_subquery = f\"(SELECT id FROM MEDICATIONS WHERE name = '{med_name}' AND dosage IS NULL)\"\n",
    "\n",
    "    disease_name = disease_name.replace(\"'\", \"''\")\n",
    "\n",
    "    disease_subquery = f\"(SELECT id FROM DISEASES WHERE name = '{disease_name}' AND condition = '{disease_condition}')\"\n",
    "\n",
    "    query = (f\"INSERT INTO TREATMENTS (medication_id, disease_id) VALUES (\\n\"\n",
    "             f\"    {med_subquery},\\n\"\n",
    "             f\"    {disease_subquery}\\n\"\n",
    "             f\");\")\n",
    "    treatment_inserts.append(query)\n",
    "\n",
    "# --- Save the INSERT queries to files ---\n",
    "\n",
    "# Save Medication INSERT queries\n",
    "with open('database/dml/insert_medications.sql', 'w') as med_file:\n",
    "    for query in medication_inserts:\n",
    "        med_file.write(query + \"\\n\")\n",
    "\n",
    "# Save Treatment INSERT queries\n",
    "with open('database/dml/insert_treatments.sql', 'w') as treat_file:\n",
    "    for query in treatment_inserts:\n",
    "        treat_file.write(query + \"\\n\")"
   ],
   "id": "bbf8cd846382baeb"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# --- Deduplication ---\n",
    "# We build a set of tuples: (bio_struct_name, disease_name, disease_condition)\n",
    "unique_relations = set()\n",
    "\n",
    "for rel in all_results[\"bio_disease\"]:\n",
    "    bio_struct = rel.get(\"biological_structure\", \"\").strip().lower()\n",
    "    disease_name = rel.get(\"disease\", \"\").strip().lower()\n",
    "    disease_condition = rel.get(\"condition\", \"\").strip().lower() if \"condition\" in rel else None\n",
    "\n",
    "    unique_relations.add((bio_struct, disease_name, disease_condition))\n",
    "\n",
    "# --- Generate INSERT queries for INVOLVEMENTS ---\n",
    "# The INVOLVEMENTS table has foreign keys for BIOLOGICAL_STRUCTURE and DISEASES.\n",
    "# We use subqueries to look up these IDs.\n",
    "involvement_inserts = []\n",
    "for bio_struct, disease_name, disease_condition in unique_relations:\n",
    "    bio_struct = bio_struct.replace(\"'\", \"''\")\n",
    "    bio_struct_subquery = f\"(SELECT id FROM BIOLOGICAL_STRUCTURE WHERE name = '{bio_struct}')\"\n",
    "\n",
    "    disease_name = disease_name.replace(\"'\", \"''\")\n",
    "    disease_subquery = f\"(SELECT id FROM DISEASES WHERE name = '{disease_name}' AND condition = '{disease_condition}')\"\n",
    "\n",
    "    query = (\n",
    "        f\"INSERT INTO INVOLVEMENTS (biological_structure_id, disease_id) VALUES (\\n\"\n",
    "        f\"    {bio_struct_subquery},\\n\"\n",
    "        f\"    {disease_subquery}\\n\"\n",
    "        f\");\"\n",
    "    )\n",
    "    involvement_inserts.append(query)\n",
    "\n",
    "# --- Save the INSERT queries to a file ---\n",
    "with open('database/dml/insert_involvements.sql', 'w') as file:\n",
    "    for query in involvement_inserts:\n",
    "        file.write(query + \"\\n\")"
   ],
   "id": "9db81e33302b0d1c"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# --- Deduplication ---\n",
    "# Each unique relation is represented as a tuple: (diagnostic_test, disease_name, disease_condition)\n",
    "unique_diagnoses = set()\n",
    "\n",
    "for rel in all_results[\"diagnosis\"]:\n",
    "    diag = rel.get(\"diagnostic_test\", \"\").strip().lower()\n",
    "    disease_name = rel.get(\"disease\", \"\").strip().lower()\n",
    "    disease_condition = rel.get(\"condition\", \"\").strip().lower() if \"condition\" in rel else None\n",
    "\n",
    "    if diag and disease_name:  # Ensure essential values exist\n",
    "        unique_diagnoses.add((diag, disease_name, disease_condition))\n",
    "\n",
    "# --- Generate INSERT queries for DIAGNOSIS ---\n",
    "diagnosis_inserts = []\n",
    "for diag, disease_name, disease_condition in unique_diagnoses:\n",
    "    # Subquery for diagnostic test\n",
    "    diag = diag.replace(\"'\", \"''\")\n",
    "    diag_subquery = f\"(SELECT id FROM DIAGNOSTIC_TESTS WHERE name = '{diag}')\"\n",
    "\n",
    "    disease_name = disease_name.replace(\"'\", \"''\")\n",
    "    disease_subquery = f\"(SELECT id FROM DISEASES WHERE name = '{disease_name}' AND condition = '{disease_condition}')\"\n",
    "\n",
    "    query = (f\"INSERT INTO DIAGNOSIS (diagnostic_test_id, disease_id) VALUES (\\n\"\n",
    "             f\"    {diag_subquery},\\n\"\n",
    "             f\"    {disease_subquery}\\n\"\n",
    "             f\");\")\n",
    "    diagnosis_inserts.append(query)\n",
    "\n",
    "# --- Save the INSERT queries to a file ---\n",
    "with open('database/dml/insert_diagnosis.sql', 'w') as diag_file:\n",
    "    for query in diagnosis_inserts:\n",
    "        diag_file.write(query + \"\\n\")"
   ],
   "id": "1827003667817e21"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
